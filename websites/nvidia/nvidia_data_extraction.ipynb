{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072fa943-c377-41cb-b01c-b68fa00b1183",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff2c691-4afe-4a05-ae8f-5bafef92de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from rich import print\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from langchain.docstore.document import Document\n",
    "from rich.console import Console\n",
    "from rich.status import Status\n",
    "from langchain.document_loaders import AsyncHtmlLoader\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba65e7e-4a7a-4483-ba1c-2547278dff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_base_path(url: str):\n",
    "    parsed_url = urlparse(url)\n",
    "    base_path = parsed_url.scheme + \"://\" + parsed_url.netloc + parsed_url.path.rsplit('/', 1)[0] + '/'\n",
    "    return base_path\n",
    "\n",
    "def find_linked_urls(url: str, tag_class_name: str='right-next', collected_urls=None):\n",
    "    if collected_urls is None:\n",
    "        collected_urls = []\n",
    "\n",
    "    # extract base_url path from url, everything except the file_name\n",
    "    base_url = get_url_base_path(url)\n",
    "\n",
    "    # extract html text from url using bs4\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    soup = bs4.BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # find href to the next url using tag_class_name\n",
    "    tag = soup.find(\"a\", {\"class\": tag_class_name})\n",
    "    if tag and 'href' in tag.attrs:\n",
    "        next_link = tag['href']\n",
    "        next_url = urljoin(base_url, next_link)\n",
    "        \n",
    "        # Check if this URL is already collected to prevent infinite loop\n",
    "        if next_url not in collected_urls:\n",
    "            collected_urls.append(next_url)\n",
    "            find_linked_urls(next_url, tag_class_name, collected_urls)\n",
    "\n",
    "    return collected_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c32c43-db58-4dd7-99cb-88e337661c8a",
   "metadata": {},
   "source": [
    "### Scraping TensorRT documentation\n",
    "\n",
    "<https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html>\n",
    "\n",
    "This is a single page document with no next-links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7569a8f6-c9ab-4960-845d-057b6ddd4b4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tensort_url = \"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html\"\n",
    "\n",
    "metadata = dict()\n",
    "metadata['source'] = tensort_url\n",
    "metadata['language'] = 'en'\n",
    "tensorrt_html = requests.get(tensort_url).text\n",
    "soup = bs4.BeautifulSoup(tensorrt_html, \"html.parser\")\n",
    "title = soup.title.text.strip()\n",
    "if title:\n",
    "    metadata['title'] = title\n",
    "contents_div = soup.find(\"article\", {\"id\": \"contents\"})\n",
    "if contents_div:\n",
    "    page_content = contents_div.text.strip()\n",
    "\n",
    "tensort_doc = Document(page_content=page_content, metadata=metadata)\n",
    "json_obj = tensort_doc.to_json()\n",
    "# print(json_obj)\n",
    "# print(tensort_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdd5391-dd27-4c62-8445-b7f96d1a5717",
   "metadata": {},
   "source": [
    "## Let's scrape the base url and all linked urls\n",
    "\n",
    "<https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dd8d4e-c72b-4d76-944a-f0fa633bcd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_url = \"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html\"\n",
    "all_linked_urls = find_linked_urls(initial_url)\n",
    "# print(all_linked_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a231a5de-8f85-4ef7-97e6-699123c2eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write urls to text file\n",
    "URLS_FILE = Path(\"./nvidia_trition_inference_server.urls.txt\")\n",
    "urls_to_write = \"\\n\".join(all_linked_urls)\n",
    "URLS_FILE.write_text(urls_to_write, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8fd6ae-4ea2-4dc5-94a3-5394a54df971",
   "metadata": {},
   "source": [
    "#### Now let's prepare LangChain Documents for each url\n",
    "\n",
    "Upon inspecting the web pages all the main content is under this `<div class=\"tex2jax_ignore mathjax_ignore section\" ../>`\n",
    "\n",
    "Function `docs_from_html_div` extracts text from this div and returns the `Document` along with `metadata` with **source** and **title**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f7b11-9f81-433c-b405-ace4b8d57ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract all text under a div with a class name\n",
    "def docs_from_html_div(url: str, div_class_name: str, lang: str='en'):\n",
    "    # use bs4 to extract the text from the url and then by div_class_name\n",
    "    metadata = dict()\n",
    "    page_content = \"\"\n",
    "    metadata['source'] = url\n",
    "    metadata['language'] = lang\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    # extract all text under the div with the class name\n",
    "    soup = bs4.BeautifulSoup(html_content, \"html.parser\")\n",
    "    title = soup.title.text.strip()\n",
    "    if title:\n",
    "        metadata['title'] = title\n",
    "    div = soup.find(\"div\", {\"class\": div_class_name})\n",
    "    if div:\n",
    "        page_content = div.text.strip()\n",
    "    return Document(page_content=page_content, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef88ecbc-e063-46e8-998c-4c94ce799cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "console = Console()\n",
    "status = Status(console=console, spinner=\"earth\", status=\"Preparing docs\")\n",
    "status.start()\n",
    "all_docs = []\n",
    "desired_class = \"tex2jax_ignore mathjax_ignore section\"\n",
    "for url in all_linked_urls:\n",
    "    status.update(status=f\"Preparing document for [i green]{url}[/i green]\")\n",
    "    doc = docs_from_html_div(url, desired_class)\n",
    "    all_docs.append(doc)\n",
    "status.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00698f7-44f1-4de3-be99-044e4f948948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b67f22-0052-4c11-916d-ea21dd7ae743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb511f1-effd-4734-b99b-17e8d659bd24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b9b6ef-5eab-462d-a368-d04950188d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee7894d-3147-43af-b82b-e133178c916d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c57505-d35c-46d4-b169-ac2aca7eb50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\n",
    "# loader = AsyncHtmlLoader(urls)\n",
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d4a4bd-9828-4db0-a49f-15e3fc94b471",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# urls = [\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html\"]\n",
    "# loader = AsyncHtmlLoader(all_linked_urls)\n",
    "# html_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d962d-b7f1-4953-8867-8c05a9e075a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from langchain.document_transformers import BeautifulSoupTransformer\n",
    "# bs_transformer = BeautifulSoupTransformer()\n",
    "# docs_transformed = bs_transformer.transform_documents(\n",
    "#     html_docs, tags_to_extract=[\"div\"]\n",
    "# )\n",
    "# docs_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c7577c-991a-4141-9c1d-d8dcbd306880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = docs_transformed[0]\n",
    "# doc.page_content.find_all("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b01eaf3-d9aa-43fe-9efd-aec91d0aa3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the desired class from the transformed content\n",
    "# desired_class = \"tex2jax_ignore mathjax_ignore section\"\n",
    "# extracted_data = []\n",
    "# for doc in docs_transformed:\n",
    "#     for tag in doc.page_content.find_all(class_=desired_class):\n",
    "#         extracted_data.append(tag.text)\n",
    "\n",
    "# # Print the extracted data\n",
    "# for data in extracted_data:\n",
    "#     print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb27f1-c88e-4648-bcb5-7bf637b1e768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e877a-248e-4158-a7b6-58722105be91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac7188-0357-4826-bb54-e866ac733784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "147f9286-ce3e-4396-a240-68e729b9837b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### HTML2Text Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b3f15-5066-4456-b079-bfc602623989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_transformers import Html2TextTransformer\n",
    "\n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(html_docs)\n",
    "docs_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84537b80-185b-4482-96cd-ef26321ae677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311",
   "language": "python",
   "name": "nlp311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
