{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c605f3d9-432f-4baf-802c-6e9d05fd1880",
   "metadata": {},
   "source": [
    "## Scrape AWS Machine Learning Blog posts\n",
    "\n",
    "### Scraping using RSS Feed\n",
    "\n",
    "In the code block below, we are parsing the RSS feed from the AWS Machine Learning blog using the `feedparser` library. For each entry in the feed, we extract the title, published date, tags, content, and URL of the blog post.\n",
    "\n",
    "We then store this information in a pandas DataFrame. Each row in the DataFrame corresponds to a single blog post.\n",
    "\n",
    "Next, we extract the trailing string from the URL of the blog post and use it as the filename for the Parquet file. This is done using the `os.path.basename` and `os.path.normpath` functions.\n",
    "\n",
    "Finally, we save the DataFrame to a Parquet file using the `to_parquet` method. We specify the `pyarrow` engine for writing the Parquet file and use Snappy compression to reduce the file size.\n",
    "\n",
    "This results in a separate Parquet file for each blog post in the RSS feed, with the filename corresponding to the trailing string in the URL of the blog post.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cece7c-7caf-4a04-8232-9828e45a7d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U requests bs4 pyarrow pandas feedparser --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d28895b-cf05-4928-8d37-e7897fdf43ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Files written to data/aws/ml_blog_posts/rss\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Files written to data/aws/ml_blog_posts/rss\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from rich import print\n",
    "\n",
    "url = \"https://aws.amazon.com/blogs/machine-learning/feed/\"\n",
    "feed = feedparser.parse(url)\n",
    "\n",
    "# Path to store extracted blog posts to\n",
    "DATADIR = Path(\"./data/aws/ml_blog_posts/rss\")\n",
    "DATADIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "for entry in feed.entries:\n",
    "    title = entry.title\n",
    "    published = entry.published\n",
    "    tags = [tag.term for tag in entry.tags]\n",
    "    content = entry.content[0].value\n",
    "    link = entry.link\n",
    "\n",
    "    # Store the extracted information in a pandas DataFrame\n",
    "    data = {\n",
    "        \"Title\": [title],\n",
    "        \"Published\": [published],\n",
    "        \"Tags\": [tags],\n",
    "        \"Content\": [content],\n",
    "        \"URL\": [link],\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Extract the trailing string from the URL\n",
    "    filename = os.path.basename(os.path.normpath(link))\n",
    "    parquet_file = Path(f\"{DATADIR}/{filename}.parquet\")\n",
    "    if not parquet_file.exists():\n",
    "        # Save the DataFrame to a Parquet file\n",
    "        print(f\"Saving: {parquet_file}\")\n",
    "        df.to_parquet(parquet_file, engine=\"pyarrow\", compression=\"snappy\")\n",
    "\n",
    "print(f\"Files written to {DATADIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dd414b-3a36-40f5-a7e7-30ea2a354929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_blog_urls(feed_url):\n",
    "    feed = feedparser.parse(feed_url)\n",
    "    blog_urls = [entry.link for entry in feed.entries]\n",
    "    return blog_urls\n",
    "\n",
    "\n",
    "feed_url = \"https://aws.amazon.com/blogs/machine-learning/feed/\"\n",
    "blog_urls = extract_blog_urls(feed_url)\n",
    "\n",
    "for url in blog_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e425f4-262d-46e6-bc58-d6a8264386e9",
   "metadata": {},
   "source": [
    "## Scraping using BeautifulSoup\n",
    "\n",
    "In the code block below, we are scraping specific blog posts from the AWS Machine Learning blog using the `requests` and `BeautifulSoup` libraries. We specify the URLs of the blog posts we want to scrape in the `urls` list.\n",
    "\n",
    "For each URL in the list, we send a GET request to the URL and parse the response using BeautifulSoup. We then locate and extract the title, metadata, authors, published date, content, and image URLs of the blog post using BeautifulSoup's `find` and `find_all` methods.\n",
    "\n",
    "We store this information in a pandas DataFrame, with each row in the DataFrame corresponding to a single blog post. We then extract the trailing string from the URL of the blog post and use it as the filename for the Parquet file.\n",
    "\n",
    "Finally, we save the DataFrame to a Parquet file using the `to_parquet` method. We specify the `pyarrow` engine for writing the Parquet file and use Snappy compression to reduce the file size.\n",
    "\n",
    "This results in a separate Parquet file for each blog post in the `urls` list, with the filename corresponding to the trailing string in the URL of the blog post. The Parquet files are saved in the specified directory (`DATADIR`).\n",
    "\n",
    "This script provides an efficient way to scrape specific blog posts from the AWS Machine Learning blog and store the scraped data in a structured format for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd9a1597-bdb7-44a0-ae24-92c87031a08f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping blog: achieve-high-performance-with-lowest-cost-for-generative-ai-inference-using-aws-inferentia2-and-aws-trainium-on-amazon-sagemaker\n",
      "Files written to data/aws/ml_blog_posts/bs\n"
     ]
    }
   ],
   "source": [
    "# Replace with the URL of the blog post you want to scrape\n",
    "urls = [\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/zero-shot-prompting-for-the-flan-t5-foundation-model-in-amazon-sagemaker-jumpstart/\",\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/deploy-amazon-sagemaker-autopilot-models-to-serverless-inference-endpoints/\",\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/best-practices-for-load-testing-amazon-sagemaker-real-time-inference-endpoints/\",\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/achieve-high-performance-with-lowest-cost-for-generative-ai-inference-using-aws-inferentia2-and-aws-trainium-on-amazon-sagemaker/\",\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/part-1-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-opportunities-based-on-usage-part-1/\",\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/part-2-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-opportunities-based-on-usage-part-2-sagemaker-notebooks-and-studio/\",\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/part-3-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-opportunities-based-on-usage-part-3-processing-and-data-wrangler-jobs/\",\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/part-4-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-opportunities-based-on-usage-part-4-training-jobs/\",\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/part-5-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-opportunities-based-on-usage-part-5-hosting/\",\n",
    "]\n",
    "\n",
    "\n",
    "DATADIR = Path(\"./data/aws/ml_blog_posts/bs\")\n",
    "DATADIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for url in urls:\n",
    "    post_name = url.split(\"/\")[-2]\n",
    "    parquet_file = Path(f\"{DATADIR}/{post_name}.parquet\")\n",
    "\n",
    "    if not parquet_file.exists():\n",
    "        print(f\"Scraping blog: {post_name}\")\n",
    "        response = requests.get(url)\n",
    "        data = {}\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Locate and extract the desired information\n",
    "            title = soup.find(\"h1\", class_=\"blog-post-title\").text.strip()\n",
    "            # metadata = soup.find('footer', class_='blog-post-meta').text.strip()\n",
    "            metadata_elements = soup.find_all(\n",
    "                \"span\", attrs={\"property\": \"articleSection\"}\n",
    "            )\n",
    "            metadata = [mdata_element.text for mdata_element in metadata_elements]\n",
    "            # print(metadata)\n",
    "\n",
    "            author_elements = soup.find_all(\"span\", attrs={\"property\": \"author\"})\n",
    "            # Extract the author names and store them in a list\n",
    "            author_names = [\n",
    "                author_element.find(\"span\", attrs={\"property\": \"name\"}).text\n",
    "                for author_element in author_elements\n",
    "            ]\n",
    "            # print(author_names)\n",
    "\n",
    "            # Extract datePublished\n",
    "            time_element = soup.find(\"time\", attrs={\"property\": \"datePublished\"})\n",
    "            date_published = time_element[\"datetime\"]\n",
    "            # print(date_published)\n",
    "\n",
    "            section_element = soup.find(\"section\", class_=\"blog-post-content\")\n",
    "            content = section_element.text.strip()\n",
    "            image_urls = [\n",
    "                img[\"src\"] for img in soup.find_all(\"img\", class_=\"alignnone\")\n",
    "            ]\n",
    "\n",
    "            # Store the extracted information in a pandas DataFrame\n",
    "            data = {\n",
    "                \"Title\": [title],\n",
    "                \"Tags\": [metadata],\n",
    "                \"Authors\": [author_names],\n",
    "                \"Published Date\": [date_published],\n",
    "                \"Content\": [content],\n",
    "                \"Image URLs\": [image_urls],\n",
    "                \"URL\": [url],\n",
    "            }\n",
    "            df = pd.DataFrame(data)\n",
    "\n",
    "            df.to_parquet(parquet_file, engine=\"pyarrow\", compression=\"snappy\")\n",
    "        else:\n",
    "            print(f\"Error fetching data: {response.status_code}\")\n",
    "\n",
    "print(f\"Files written to {DATADIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4507fb-e102-4797-9bd6-dac002d959fe",
   "metadata": {},
   "source": [
    "### List all extracted files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560a1ef0-4220-416f-8ad9-212f6a6fc8f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use rglob to recursively find all files\n",
    "file_list = list(DATADIR.rglob(\"*.parquet\"))\n",
    "\n",
    "# Print the list of files\n",
    "print(file_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
