{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c605f3d9-432f-4baf-802c-6e9d05fd1880",
   "metadata": {},
   "source": [
    "## Scrape AWS Machine Learning Blog posts\n",
    "\n",
    "### Scraping using RSS Feed\n",
    "\n",
    "In the below code block, we parse the RSS feed from the AWS Machine Learning blog using the `feedparser` library. For each entry in the feed, we extract the title, authors, published_date, tags, content, and URL of the blog post.\n",
    "\n",
    "We then store this information in a pandas DataFrame. Each row in the DataFrame corresponds to a single blog post.\n",
    "\n",
    "Next, we extract the trailing string from the URL of the blog post and use it as the filename for the Parquet file. This is done using the `os.path.basename` and `os.path.normpath` functions.\n",
    "\n",
    "We save the DataFrame to a Parquet file using the `to_parquet` method. We specify the `pyarrow` engine for writing the Parquet file and use Snappy compression to reduce the file size.\n",
    "\n",
    "We also store the extracted links along with extracted datetime to `pickledb`.\n",
    "This is to ensure we are not scraping the same links repeatedly.\n",
    "\n",
    "This results in a separate Parquet file for each blog post in the RSS feed, with the filename corresponding to the trailing string in the URL of the blog post.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d0e17",
   "metadata": {},
   "source": [
    "## Install Pre requisties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42cece7c-7caf-4a04-8232-9828e45a7d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U requests bs4 pyarrow pandas feedparser pickledb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d28895b-cf05-4928-8d37-e7897fdf43ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from rich import print\n",
    "import pickledb\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "url = \"https://aws.amazon.com/blogs/machine-learning/feed/\"\n",
    "feed = feedparser.parse(url)\n",
    "\n",
    "# Path to store extracted blog posts to\n",
    "DATADIR = Path(\"./data/aws/ml_blog_posts/rss\")\n",
    "DATADIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Path to store extracted blog posts to pickledb\n",
    "DB_DIR = Path(\"db\")\n",
    "DB_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac4c9499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Links saved to: db/blogposts.db\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Links saved to: db/blogposts.db\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize pickledb\n",
    "db = pickledb.load(f\"{DB_DIR}/blogposts.db\", False)\n",
    "\n",
    "for entry in feed.entries:\n",
    "    link = entry.link\n",
    "    # Check if this entry's URL is already in the pickledb\n",
    "    if db.get(link):\n",
    "        # This entry is already in the db, so skip it\n",
    "        # print(f\"Skipping: {link}\")\n",
    "        continue\n",
    "\n",
    "    title = entry.title\n",
    "    published = entry.published\n",
    "    tags = [tag.term for tag in entry.tags]\n",
    "    authors = [author.name for author in entry.authors]\n",
    "    # Extract the content from the HTML\n",
    "    content = BeautifulSoup(entry.content[0].value, \"html.parser\").get_text()\n",
    "    \n",
    "\n",
    "    # Store the extracted information in a pandas DataFrame\n",
    "    data = {\n",
    "        \"title\": [title],\n",
    "        \"tags\": [tags],\n",
    "        \"authors\": [authors],\n",
    "        \"published_date\": [published],\n",
    "        \"content\": [content],\n",
    "        \"source\": [link],\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Extract the trailing string from the URL\n",
    "    filename = os.path.basename(os.path.normpath(link))\n",
    "    parquet_file = Path(f\"{DATADIR}/{filename}.parquet\")\n",
    "    if not parquet_file.exists():\n",
    "        # Save the DataFrame to a Parquet file\n",
    "        # print(f\"Saving: {parquet_file}\")\n",
    "        df.to_parquet(parquet_file, engine=\"pyarrow\", compression=\"snappy\")\n",
    "        # Store the URL in the pickledb\n",
    "        db.set(link, datetime.now(timezone.utc).isoformat())\n",
    "        db.dump()\n",
    "\n",
    "print(f\"Links saved to: {DB_DIR}/blogposts.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e425f4-262d-46e6-bc58-d6a8264386e9",
   "metadata": {},
   "source": [
    "## Scraping using BeautifulSoup\n",
    "\n",
    "In the code block below, we are scraping specific blog posts from the AWS Machine Learning blog using the `requests` and `BeautifulSoup` libraries. We specify the URLs of the blog posts we want to scrape in the `urls` list.\n",
    "\n",
    "For each URL in the list, we send a GET request to the URL and parse the response using BeautifulSoup. We then locate and extract the title, metadata, authors, published date, content, and image URLs of the blog post using BeautifulSoup's `find` and `find_all` methods.\n",
    "\n",
    "We store this information in a pandas DataFrame, with each row in the DataFrame corresponding to a single blog post. We then extract the trailing string from the URL of the blog post and use it as the filename for the Parquet file.\n",
    "\n",
    "Finally, we save the DataFrame to a Parquet file using the `to_parquet` method. We specify the `pyarrow` engine for writing the Parquet file and use Snappy compression to reduce the file size.\n",
    "\n",
    "This results in a separate Parquet file for each blog post in the `urls` list, with the filename corresponding to the trailing string in the URL of the blog post. The Parquet files are saved in the specified directory (`DATADIR`).\n",
    "\n",
    "This script provides an efficient way to scrape specific blog posts from the AWS Machine Learning blog and store the scraped data in a structured format for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd9a1597-bdb7-44a0-ae24-92c87031a08f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Links stored to db db/blogposts.db\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Links stored to db db/blogposts.db\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pickledb if exists\n",
    "db = pickledb.load(f\"{DB_DIR}/blogposts.db\", False)\n",
    "\n",
    "# Replace with the URL of the blog post you want to scrape\n",
    "urls = [\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/zero-shot-prompting-for-the-flan-t5-foundation-model-in-amazon-sagemaker-jumpstart/\",\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/deploy-amazon-sagemaker-autopilot-models-to-serverless-inference-endpoints/\",\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/best-practices-for-load-testing-amazon-sagemaker-real-time-inference-endpoints/\",\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/achieve-high-performance-with-lowest-cost-for-generative-ai-inference-using-aws-inferentia2-and-aws-trainium-on-amazon-sagemaker/\",\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/part-1-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-opportunities-based-on-usage-part-1/\",\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/part-2-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-opportunities-based-on-usage-part-2-sagemaker-notebooks-and-studio/\",\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/part-3-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-opportunities-based-on-usage-part-3-processing-and-data-wrangler-jobs/\",\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/part-4-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-opportunities-based-on-usage-part-4-training-jobs/\",\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/part-5-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-opportunities-based-on-usage-part-5-hosting/\",\n",
    "]\n",
    "\n",
    "\n",
    "DATADIR = Path(\"./data/aws/ml_blog_posts/bs\")\n",
    "DATADIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for url in urls:\n",
    "    # Check if this entry's URL is already in the pickledb\n",
    "    if db.get(url):\n",
    "        # This entry is already in the db, so skip it\n",
    "        # print(f\"Skipping: {link}\")\n",
    "        continue    \n",
    "    post_name = url.split(\"/\")[-2]\n",
    "    parquet_file = Path(f\"{DATADIR}/{post_name}.parquet\")\n",
    "\n",
    "    if not parquet_file.exists():\n",
    "        print(f\"Scraping blog: {post_name}\")\n",
    "        response = requests.get(url)\n",
    "        data = {}\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Locate and extract the desired information\n",
    "            title = soup.find(\"h1\", class_=\"blog-post-title\").text.strip()\n",
    "            # metadata = soup.find('footer', class_='blog-post-meta').text.strip()\n",
    "            metadata_elements = soup.find_all(\n",
    "                \"span\", attrs={\"property\": \"articleSection\"}\n",
    "            )\n",
    "            metadata = [mdata_element.text for mdata_element in metadata_elements]\n",
    "            # print(metadata)\n",
    "\n",
    "            author_elements = soup.find_all(\"span\", attrs={\"property\": \"author\"})\n",
    "            # Extract the author names and store them in a list\n",
    "            author_names = [\n",
    "                author_element.find(\"span\", attrs={\"property\": \"name\"}).text\n",
    "                for author_element in author_elements\n",
    "            ]\n",
    "            # print(author_names)\n",
    "\n",
    "            # Extract datePublished\n",
    "            time_element = soup.find(\"time\", attrs={\"property\": \"datePublished\"})\n",
    "            date_published = time_element[\"datetime\"]\n",
    "            # print(date_published)\n",
    "\n",
    "            section_element = soup.find(\"section\", class_=\"blog-post-content\")\n",
    "            content = section_element.text.strip()\n",
    "            image_urls = [\n",
    "                img[\"src\"] for img in soup.find_all(\"img\", class_=\"alignnone\")\n",
    "            ]\n",
    "\n",
    "            # Store the extracted information in a pandas DataFrame\n",
    "            data = {\n",
    "                \"title\": [title],\n",
    "                \"tags\": [metadata],\n",
    "                \"authors\": [author_names],\n",
    "                \"published_date\": [date_published],\n",
    "                \"content\": [content],\n",
    "                \"source\": [url],\n",
    "            }\n",
    "            df = pd.DataFrame(data)\n",
    "\n",
    "            df.to_parquet(parquet_file, engine=\"pyarrow\", compression=\"snappy\")\n",
    "            # Store the URL in the pickledb\n",
    "            db.set(url, datetime.now(timezone.utc).isoformat())\n",
    "            db.dump()\n",
    "        else:\n",
    "            print(f\"Error fetching data: {response.status_code}\")\n",
    "\n",
    "print(f\"Links stored to db {DB_DIR}/blogposts.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4507fb-e102-4797-9bd6-dac002d959fe",
   "metadata": {},
   "source": [
    "### List all extracted files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "560a1ef0-4220-416f-8ad9-212f6a6fc8f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/aws/ml_blog_posts/bs/deploy-amazon-sagemaker-autopilot-models-to-serverless-inference-endpoints</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">.parquet'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/aws/ml_blog_posts/bs/part-1-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-oppo</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rtunities-based-on-usage-part-1.parquet'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/aws/ml_blog_posts/bs/part-2-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-oppo</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rtunities-based-on-usage-part-2-sagemaker-notebooks-and-studio.parquet'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/aws/ml_blog_posts/bs/part-3-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-oppo</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rtunities-based-on-usage-part-3-processing-and-data-wrangler-jobs.parquet'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/aws/ml_blog_posts/bs/part-5-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-oppo</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rtunities-based-on-usage-part-5-hosting.parquet'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/aws/ml_blog_posts/bs/part-4-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-oppo</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rtunities-based-on-usage-part-4-training-jobs.parquet'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/aws/ml_blog_posts/bs/achieve-high-performance-with-lowest-cost-for-generative-ai-inference-usin</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">g-aws-inferentia2-and-aws-trainium-on-amazon-sagemaker.parquet'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/aws/ml_blog_posts/bs/best-practices-for-load-testing-amazon-sagemaker-real-time-inference-endpo</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ints.parquet'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/aws/ml_blog_posts/bs/zero-shot-prompting-for-the-flan-t5-foundation-model-in-amazon-sagemaker-j</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">umpstart.parquet'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/aws/ml_blog_posts/bs/deploy-amazon-sagemaker-autopilot-models-to-serverless-inference-endpoints\u001b[0m\n",
       "\u001b[32m.parquet'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/aws/ml_blog_posts/bs/part-1-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-oppo\u001b[0m\n",
       "\u001b[32mrtunities-based-on-usage-part-1.parquet'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/aws/ml_blog_posts/bs/part-2-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-oppo\u001b[0m\n",
       "\u001b[32mrtunities-based-on-usage-part-2-sagemaker-notebooks-and-studio.parquet'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/aws/ml_blog_posts/bs/part-3-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-oppo\u001b[0m\n",
       "\u001b[32mrtunities-based-on-usage-part-3-processing-and-data-wrangler-jobs.parquet'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/aws/ml_blog_posts/bs/part-5-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-oppo\u001b[0m\n",
       "\u001b[32mrtunities-based-on-usage-part-5-hosting.parquet'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/aws/ml_blog_posts/bs/part-4-analyze-amazon-sagemaker-spend-and-determine-cost-optimization-oppo\u001b[0m\n",
       "\u001b[32mrtunities-based-on-usage-part-4-training-jobs.parquet'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/aws/ml_blog_posts/bs/achieve-high-performance-with-lowest-cost-for-generative-ai-inference-usin\u001b[0m\n",
       "\u001b[32mg-aws-inferentia2-and-aws-trainium-on-amazon-sagemaker.parquet'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/aws/ml_blog_posts/bs/best-practices-for-load-testing-amazon-sagemaker-real-time-inference-endpo\u001b[0m\n",
       "\u001b[32mints.parquet'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/aws/ml_blog_posts/bs/zero-shot-prompting-for-the-flan-t5-foundation-model-in-amazon-sagemaker-j\u001b[0m\n",
       "\u001b[32mumpstart.parquet'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use rglob to recursively find all files\n",
    "file_list = list(DATADIR.rglob(\"*.parquet\"))\n",
    "\n",
    "# Print the list of files\n",
    "print(file_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
